{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c5f21cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb51befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.6.11\n",
      "Pycolmap version 0.3.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)\n",
    "\n",
    "LOCAL_FEATURE = 'LoFTR'\n",
    "device=torch.device('cuda')\n",
    "# Can be LoFTR, KeyNetAffNetHardNet, or DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd83b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "\n",
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.\n",
    "    img = K.color.bgr_to_rgb(img.to(device))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "127f7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, model,\n",
    "                    device =  torch.device('cpu')):\n",
    "    model = model.eval()\n",
    "    model= model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all\n",
    "\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "\n",
    "    model = timm.create_model('tf_efficientnet_b7',\n",
    "                              checkpoint_path='/home/jsmoon/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    model.eval()\n",
    "    descs = get_global_desc(fnames, model, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d1f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d6f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ac0df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyNetAffNetHardNet(KF.LocalFeature):\n",
    "    \"\"\"Convenience module, which implements KeyNet detector + AffNet + HardNet descriptor.\n",
    "\n",
    "    .. image:: _static/img/keynet_affnet.jpg\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 5000,\n",
    "        upright: bool = False,\n",
    "        device = torch.device('cpu'),\n",
    "        scale_laf: float = 1.0,\n",
    "    ):\n",
    "        ori_module = KF.PassLAF() if upright else KF.LAFOrienter(angle_detector=KF.OriNet(False)).eval()\n",
    "        if not upright:\n",
    "            weights = torch.load('/kaggle/input/kornia-local-feature-weights/OriNet.pth')['state_dict']\n",
    "            ori_module.angle_detector.load_state_dict(weights)\n",
    "        detector = KF.KeyNetDetector(\n",
    "            False, num_features=num_features, ori_module=ori_module, aff_module=KF.LAFAffNetShapeEstimator(False).eval()\n",
    "        ).to(device)\n",
    "        kn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/keynet_pytorch.pth')['state_dict']\n",
    "        detector.model.load_state_dict(kn_weights)\n",
    "        affnet_weights = torch.load('/kaggle/input/kornia-local-feature-weights/AffNet.pth')['state_dict']\n",
    "        detector.aff.load_state_dict(affnet_weights)\n",
    "        \n",
    "        hardnet = KF.HardNet(False).eval()\n",
    "        hn_weights = torch.load('/kaggle/input/kornia-local-feature-weights/HardNetLib.pth')['state_dict']\n",
    "        hardnet.load_state_dict(hn_weights)\n",
    "        descriptor = KF.LAFDescriptor(hardnet, patch_size=32, grayscale_descriptor=True).to(device)\n",
    "        super().__init__(detector, descriptor, scale_laf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56dd17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_features(img_fnames,\n",
    "                    num_feats = 2048,\n",
    "                    upright = False,\n",
    "                    device=torch.device('cpu'),\n",
    "                    feature_dir = '.featureout',\n",
    "                    resize_small_edge_to = 600):\n",
    "    if LOCAL_FEATURE == 'DISK':\n",
    "        # Load DISK from Kaggle models so it can run when the notebook is offline.\n",
    "        disk = KF.DISK().to(device)\n",
    "        pretrained_dict = torch.load('/kaggle/input/disk/pytorch/depth-supervision/1/loftr_outdoor.ckpt', map_location=device)\n",
    "        disk.load_state_dict(pretrained_dict['extractor'])\n",
    "        disk.eval()\n",
    "    if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n",
    "        feature = KeyNetAffNetHardNet(num_feats, upright, device).to(device).eval()\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='w') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in progress_bar(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                timg = load_torch_image(img_path, device=device)\n",
    "                H, W = timg.shape[2:]\n",
    "                if resize_small_edge_to is None:\n",
    "                    timg_resized = timg\n",
    "                else:\n",
    "                    timg_resized = K.geometry.resize(timg, resize_small_edge_to, antialias=True)\n",
    "                    print(f'Resized {timg.shape} to {timg_resized.shape} (resize_small_edge_to={resize_small_edge_to})')\n",
    "                h, w = timg_resized.shape[2:]\n",
    "                if LOCAL_FEATURE == 'DISK':\n",
    "                    features = disk(timg_resized, num_feats, pad_if_not_divisible=True)[0]\n",
    "                    kps1, descs = features.keypoints, features.descriptors\n",
    "                    \n",
    "                    lafs = KF.laf_from_center_scale_ori(kps1[None], torch.ones(1, len(kps1), 1, 1, device=device))\n",
    "                if LOCAL_FEATURE == 'KeyNetAffNetHardNet':\n",
    "                    lafs, resps, descs = feature(K.color.rgb_to_grayscale(timg_resized))\n",
    "                lafs[:,:,0,:] *= float(W) / float(w)\n",
    "                lafs[:,:,1,:] *= float(H) / float(h)\n",
    "                desc_dim = descs.shape[-1]\n",
    "                kpts = KF.get_laf_center(lafs).reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = descs.reshape(-1, desc_dim).detach().cpu().numpy()\n",
    "                f_laf[key] = lafs.detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return\n",
    "\n",
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def match_features(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=15, \n",
    "                   force_mutual = True,\n",
    "                   matching_alg='smnn'\n",
    "                  ):\n",
    "    assert matching_alg in ['smnn', 'adalam']\n",
    "    with h5py.File(f'{feature_dir}/lafs.h5', mode='r') as f_laf, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "                    idx1, idx2 = pair_idx\n",
    "                    fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "                    key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "                    lafs1 = torch.from_numpy(f_laf[key1][...]).to(device)\n",
    "                    lafs2 = torch.from_numpy(f_laf[key2][...]).to(device)\n",
    "                    desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "                    desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "                    if matching_alg == 'adalam':\n",
    "                        img1, img2 = cv2.imread(fname1), cv2.imread(fname2)\n",
    "                        hw1, hw2 = img1.shape[:2], img2.shape[:2]\n",
    "                        adalam_config = KF.adalam.get_adalam_default_config()\n",
    "                        #adalam_config['orientation_difference_threshold'] = None\n",
    "                        #adalam_config['scale_rate_threshold'] = None\n",
    "                        adalam_config['force_seed_mnn']= False\n",
    "                        adalam_config['search_expansion'] = 16\n",
    "                        adalam_config['ransac_iters'] = 128\n",
    "                        adalam_config['device'] = device\n",
    "                        dists, idxs = KF.match_adalam(desc1, desc2,\n",
    "                                                      lafs1, lafs2, # Adalam takes into account also geometric information\n",
    "                                                      hw1=hw1, hw2=hw2,\n",
    "                                                      config=adalam_config) # Adalam also benefits from knowing image size\n",
    "                    else:\n",
    "                        dists, idxs = KF.match_smnn(desc1, desc2, 0.98)\n",
    "                    if len(idxs)  == 0:\n",
    "                        continue\n",
    "                    # Force mutual nearest neighbors\n",
    "                    if force_mutual:\n",
    "                        first_indices = get_unique_idxs(idxs[:,1])\n",
    "                        idxs = idxs[first_indices]\n",
    "                        dists = dists[first_indices]\n",
    "                    n_matches = len(idxs)\n",
    "                    if False:\n",
    "                        print (f'{key1}-{key2}: {n_matches} matches')\n",
    "                    group  = f_match.require_group(key1)\n",
    "                    if n_matches >= min_matches:\n",
    "                         group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "    return\n",
    "\n",
    "def match_loftr(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout_loftr',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=15, resize_to_ = (640, 480)):\n",
    "    matcher = KF.LoFTR(pretrained=None)\n",
    "    matcher.load_state_dict(torch.load('/home/jsmoon/kaggle/input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt')['state_dict'])\n",
    "    matcher = matcher.to(device).eval()\n",
    "\n",
    "    # First we do pairwise matching, and then extract \"keypoints\" from loftr matches.\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='w') as f_match:\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            # Load img1\n",
    "            timg1 = K.color.rgb_to_grayscale(load_torch_image(fname1, device=device))\n",
    "            H1, W1 = timg1.shape[2:]\n",
    "            if H1 < W1:\n",
    "                resize_to = resize_to_[1], resize_to_[0]\n",
    "            else:\n",
    "                resize_to = resize_to_\n",
    "            timg_resized1 = K.geometry.resize(timg1, resize_to, antialias=True)\n",
    "            h1, w1 = timg_resized1.shape[2:]\n",
    "\n",
    "            # Load img2\n",
    "            timg2 = K.color.rgb_to_grayscale(load_torch_image(fname2, device=device))\n",
    "            H2, W2 = timg2.shape[2:]\n",
    "            if H2 < W2:\n",
    "                resize_to2 = resize_to[1], resize_to[0]\n",
    "            else:\n",
    "                resize_to2 = resize_to_\n",
    "            timg_resized2 = K.geometry.resize(timg2, resize_to2, antialias=True)\n",
    "            h2, w2 = timg_resized2.shape[2:]\n",
    "            with torch.inference_mode():\n",
    "                input_dict = {\"image0\": timg_resized1,\"image1\": timg_resized2}\n",
    "                correspondences = matcher(input_dict)\n",
    "            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n",
    "            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n",
    "\n",
    "            mkpts0[:,0] *= float(W1) / float(w1)\n",
    "            mkpts0[:,1] *= float(H1) / float(h1)\n",
    "\n",
    "            mkpts1[:,0] *= float(W2) / float(w2)\n",
    "            mkpts1[:,1] *= float(H2) / float(h2)\n",
    "\n",
    "            n_matches = len(mkpts1)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=np.concatenate([mkpts0, mkpts1], axis=1))\n",
    "\n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(f'{feature_dir}/matches_loftr.h5', mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return\n",
    "\n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, 'simple-radial', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "038b219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/home/jsmoon/kaggle/input/image-matching-challenge-2023/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f00fe173",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "with open(f'{src}/train_labels.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            dataset, scene, image, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2df0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urban / kyiv-puppet-theater -> 26 images\n",
      "heritage / dioscuri -> 174 images\n",
      "heritage / cyprus -> 30 images\n",
      "heritage / wall -> 43 images\n",
      "haiper / bike -> 15 images\n",
      "haiper / chairs -> 16 images\n",
      "haiper / fountain -> 23 images\n"
     ]
    }
   ],
   "source": [
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02739933",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_results = {}\n",
    "timings = {\"shortlisting\":[],\n",
    "           \"feature_detection\": [],\n",
    "           \"feature_matching\":[],\n",
    "           \"RANSAC\": [],\n",
    "           \"Reconstruction\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "add67377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59f6c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:  urban\n",
      "scene:  kyiv-puppet-theater\n",
      "img_dir:  /home/jsmoon/kaggle/input/image-matching-challenge-2023/train/urban/kyiv-puppet-theater/images\n",
      "luck\n",
      "Got 26 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████▌                     | 13/26 [00:01<00:01,  8.99it/s]"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import copy\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"dataset: \", dataset)\n",
    "    if dataset not in out_results:\n",
    "        out_results[dataset] = {}\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(\"scene: \", scene)\n",
    "        # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "        # You may want to run this on the training data in that case?\n",
    "        img_dir = f'{src}/{dataset}/{scene}/images'\n",
    "        print(\"img_dir: \", img_dir)\n",
    "        if not os.path.exists(img_dir):\n",
    "            continue\n",
    "        # Wrap the meaty part in a try-except block.\n",
    "        try:\n",
    "            print(\"luck\")\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "            t=time()\n",
    "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                                  sim_th = 0.5, # should be strict\n",
    "                                  min_pairs = 20, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                                  exhaustive_if_less = 20,\n",
    "                                  device=device)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            t=time()\n",
    "            if LOCAL_FEATURE != 'LoFTR':\n",
    "                detect_features(img_fnames, \n",
    "                                2048,\n",
    "                                feature_dir=feature_dir,\n",
    "                                upright=True,\n",
    "                                device=device,\n",
    "                                resize_small_edge_to=600\n",
    "                               )\n",
    "                gc.collect()\n",
    "                t=time() -t \n",
    "                timings['feature_detection'].append(t)\n",
    "                print(f'Features detected in  {t:.4f} sec')\n",
    "                t=time()\n",
    "                match_features(img_fnames, index_pairs, feature_dir=feature_dir,device=device)\n",
    "            else:\n",
    "                match_loftr(img_fnames, index_pairs, feature_dir=feature_dir, device=device, resize_to_=(600, 800))\n",
    "            t=time() -t \n",
    "            timings['feature_matching'].append(t)\n",
    "            print(f'Features matched in  {t:.4f} sec')\n",
    "            database_path = f'{feature_dir}/colmap.db'\n",
    "            if os.path.isfile(database_path):\n",
    "                os.remove(database_path)\n",
    "            gc.collect()\n",
    "            import_into_colmap(img_dir, feature_dir=feature_dir,database_path=database_path)\n",
    "            output_path = f'{feature_dir}/colmap_rec_{LOCAL_FEATURE}'\n",
    "\n",
    "            t=time()\n",
    "            pycolmap.match_exhaustive(database_path)\n",
    "            t=time() - t \n",
    "            timings['RANSAC'].append(t)\n",
    "            print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "            t=time()\n",
    "            # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "            mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "            mapper_options.min_model_size = 3\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "            print(maps)\n",
    "            #clear_output(wait=False)\n",
    "            t=time() - t\n",
    "            timings['Reconstruction'].append(t)\n",
    "            print(f'Reconstruction done in  {t:.4f} sec')\n",
    "            imgs_registered  = 0\n",
    "            best_idx = None\n",
    "            print (\"Looking for the best reconstruction\")\n",
    "            if isinstance(maps, dict):\n",
    "                for idx1, rec in maps.items():\n",
    "                    print (idx1, rec.summary())\n",
    "                    if len(rec.images) > imgs_registered:\n",
    "                        imgs_registered = len(rec.images)\n",
    "                        best_idx = idx1\n",
    "            if best_idx is not None:\n",
    "                print (maps[best_idx].summary())\n",
    "                for k, im in maps[best_idx].images.items():\n",
    "                    key1 = f'{dataset}/{scene}/images/{im.name}'\n",
    "                    out_results[dataset][scene][key1] = {}\n",
    "                    out_results[dataset][scene][key1][\"R\"] = copy.deepcopy(im.rotmat())\n",
    "                    out_results[dataset][scene][key1][\"t\"] = copy.deepcopy(im.tvec)\n",
    "            print(f'Registered: {dataset} / {scene} -> {len(out_results[dataset][scene])} images')\n",
    "            print(f'Total: {dataset} / {scene} -> {len(data_dict[dataset][scene])} images')\n",
    "            gc.collect()\n",
    "        except:\n",
    "            logging.exception(\"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3759dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(out_results, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46865dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
